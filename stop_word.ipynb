{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stopword List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABOUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>YOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>YOUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>YOURS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>YOURSELF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>YOURSELVES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Stopword List\n",
       "0           ABOUT\n",
       "1           ABOVE\n",
       "2           AFTER\n",
       "3           AGAIN\n",
       "4             ALL\n",
       "..            ...\n",
       "116           YOU\n",
       "117          YOUR\n",
       "118         YOURS\n",
       "119      YOURSELF\n",
       "120    YOURSELVES\n",
       "\n",
       "[121 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Stope text File\n",
    "data =pd.read_csv('D:\\ML & Data Science INTRN\\StopWords_Generic.csv')\n",
    "data.head(121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stopword List\\n',\n",
       " 'ABOUT\\n',\n",
       " 'ABOVE\\n',\n",
       " 'AFTER\\n',\n",
       " 'AGAIN\\n',\n",
       " 'ALL\\n',\n",
       " 'AM\\n',\n",
       " 'AMONG\\n',\n",
       " 'AN\\n',\n",
       " 'AND\\n',\n",
       " 'ANY\\n',\n",
       " 'ARE\\n',\n",
       " 'AS\\n',\n",
       " 'AT\\n',\n",
       " 'BE\\n',\n",
       " 'BECAUSE\\n',\n",
       " 'BEEN\\n',\n",
       " 'BEFORE\\n',\n",
       " 'BEING\\n',\n",
       " 'BELOW\\n',\n",
       " 'BETWEEN\\n',\n",
       " 'BOTH\\n',\n",
       " 'BUT\\n',\n",
       " 'BY\\n',\n",
       " 'CAN\\n',\n",
       " 'DID\\n',\n",
       " 'DO\\n',\n",
       " 'DOES\\n',\n",
       " 'DOING\\n',\n",
       " 'DOWN\\n',\n",
       " 'DURING\\n',\n",
       " 'EACH\\n',\n",
       " 'FEW\\n',\n",
       " 'FOR\\n',\n",
       " 'FROM\\n',\n",
       " 'FURTHER\\n',\n",
       " 'HAD\\n',\n",
       " 'HAS\\n',\n",
       " 'HAVE\\n',\n",
       " 'HAVING\\n',\n",
       " 'HE\\n',\n",
       " 'HER\\n',\n",
       " 'HERE\\n',\n",
       " 'HERS\\n',\n",
       " 'HERSELF\\n',\n",
       " 'HIM\\n',\n",
       " 'HIMSELF\\n',\n",
       " 'HIS\\n',\n",
       " 'HOW\\n',\n",
       " 'IF\\n',\n",
       " 'IN\\n',\n",
       " 'INTO\\n',\n",
       " 'IS\\n',\n",
       " 'IT\\n',\n",
       " 'ITS\\n',\n",
       " 'ITSELF\\n',\n",
       " 'JUST\\n',\n",
       " 'ME\\n',\n",
       " 'MORE\\n',\n",
       " 'MOST\\n',\n",
       " 'MY\\n',\n",
       " 'MYSELF\\n',\n",
       " 'NO\\n',\n",
       " 'NOR\\n',\n",
       " 'NOT\\n',\n",
       " 'NOW\\n',\n",
       " 'OF\\n',\n",
       " 'OFF\\n',\n",
       " 'ON\\n',\n",
       " 'ONCE\\n',\n",
       " 'ONLY\\n',\n",
       " 'OR\\n',\n",
       " 'OTHER\\n',\n",
       " 'OUR\\n',\n",
       " 'OURS\\n',\n",
       " 'OURSELVES\\n',\n",
       " 'OUT\\n',\n",
       " 'OVER\\n',\n",
       " 'OWN\\n',\n",
       " 'SAME\\n',\n",
       " 'SHE\\n',\n",
       " 'SHOULD\\n',\n",
       " 'SO\\n',\n",
       " 'SOME\\n',\n",
       " 'SUCH\\n',\n",
       " 'THAN\\n',\n",
       " 'THAT\\n',\n",
       " 'THE\\n',\n",
       " 'THEIR\\n',\n",
       " 'THEIRS\\n',\n",
       " 'THEM\\n',\n",
       " 'THEMSELVES\\n',\n",
       " 'THEN\\n',\n",
       " 'THERE\\n',\n",
       " 'THESE\\n',\n",
       " 'THEY\\n',\n",
       " 'THIS\\n',\n",
       " 'THOSE\\n',\n",
       " 'THROUGH\\n',\n",
       " 'TO\\n',\n",
       " 'TOO\\n',\n",
       " 'UNDER\\n',\n",
       " 'UNTIL\\n',\n",
       " 'UP\\n',\n",
       " 'VERY\\n',\n",
       " 'WAS\\n',\n",
       " 'WE\\n',\n",
       " 'WERE\\n',\n",
       " 'WHAT\\n',\n",
       " 'WHEN\\n',\n",
       " 'WHERE\\n',\n",
       " 'WHICH\\n',\n",
       " 'WHILE\\n',\n",
       " 'WHO\\n',\n",
       " 'WHOM\\n',\n",
       " 'WHY\\n',\n",
       " 'WITH\\n',\n",
       " 'YOU\\n',\n",
       " 'YOUR\\n',\n",
       " 'YOURS\\n',\n",
       " 'YOURSELF\\n',\n",
       " 'YOURSELVES\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open('D:\\ML & Data Science INTRN\\StopWords_Generic.csv')\n",
    "my_lines_list=file.readlines()\n",
    "my_lines_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abov\n",
      "after\n",
      "again\n",
      "all\n",
      "AM\n",
      "among\n",
      "AN\n",
      "and\n",
      "ani\n",
      "are\n",
      "AS\n",
      "AT\n",
      "BE\n",
      "becaus\n",
      "been\n",
      "befor\n",
      "be\n",
      "below\n",
      "between\n",
      "both\n",
      "but\n",
      "BY\n",
      "can\n",
      "did\n",
      "DO\n",
      "doe\n",
      "do\n",
      "down\n",
      "dure\n",
      "each\n",
      "few\n",
      "for\n",
      "from\n",
      "further\n",
      "had\n",
      "ha\n",
      "have\n",
      "have\n",
      "HE\n",
      "her\n",
      "here\n",
      "her\n",
      "herself\n",
      "him\n",
      "himself\n",
      "hi\n",
      "how\n",
      "IF\n",
      "IN\n",
      "into\n",
      "IS\n",
      "IT\n",
      "it\n",
      "itself\n",
      "just\n",
      "ME\n",
      "more\n",
      "most\n",
      "MY\n",
      "myself\n",
      "NO\n",
      "nor\n",
      "not\n",
      "now\n",
      "OF\n",
      "off\n",
      "ON\n",
      "onc\n",
      "onli\n",
      "OR\n",
      "other\n",
      "our\n",
      "our\n",
      "ourselv\n",
      "out\n",
      "over\n",
      "own\n",
      "same\n",
      "she\n",
      "should\n",
      "SO\n",
      "some\n",
      "such\n",
      "than\n",
      "that\n",
      "the\n",
      "their\n",
      "their\n",
      "them\n",
      "themselv\n",
      "then\n",
      "there\n",
      "these\n",
      "they\n",
      "thi\n",
      "those\n",
      "through\n",
      "TO\n",
      "too\n",
      "under\n",
      "until\n",
      "UP\n",
      "veri\n",
      "wa\n",
      "WE\n",
      "were\n",
      "what\n",
      "when\n",
      "where\n",
      "which\n",
      "while\n",
      "who\n",
      "whom\n",
      "whi\n",
      "with\n",
      "you\n",
      "your\n",
      "your\n",
      "yourself\n",
      "yourselv\n"
     ]
    }
   ],
   "source": [
    "# Function nltk.tokenize import word_tokenize from CSV file\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "input_str=\"ABOVE AFTER AGAIN ALL AM AMONG AN AND ANY ARE AS AT BE BECAUSE BEEN BEFORE BEING BELOW BETWEEN BOTH BUT BY CAN DID DO DOES DOING DOWN DURING EACH FEW FOR FROM FURTHER HAD HAS HAVE HAVING HE HER HERE HERS HERSELF HIM HIMSELF HIS HOW IF IN INTO IS IT ITS ITSELF JUST ME MORE MOST MY MYSELF NO NOR NOT NOW OF OFF ON ONCE ONLY OR OTHER OUR OURS OURSELVES OUT OVER OWN SAME SHE SHOULD SO SOME SUCH THAN THAT THE THEIR THEIRS THEM THEMSELVES THEN THERE THESE THEY THIS THOSE THROUGH TO TOO UNDER UNTIL UP VERY WAS WE WERE WHAT WHEN WHERE WHICH WHILE WHO WHOM WHY WITH YOU YOUR YOURS YOURSELF YOURSELVES\"\n",
    "input_str=nltk.word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
